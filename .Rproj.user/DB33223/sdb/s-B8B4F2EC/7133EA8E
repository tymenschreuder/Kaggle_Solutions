{
    "collab_server" : "",
    "contents" : "\n\n# Past Kaggle solutions from winners\nThis is a collection of solutions for Kaggle competitions and a summary of useful skills that I have learned from those solutions. I divided the contents into two sections, the first section is my summary of those winner solutions and the second section is the collection of those solutions and some useful tutorials.\n\n\n## Section one: my summary of those winner solutions\n\n#### General workflow  \n1. Computing environment setup\n2. Exploratory data analysis \n2. A quick benchmark run\n2. Data\tpreprocessing\n3. Feature enginnering\n4. Feature selection\n5. Model evaluation and selection\n6. Parameter turning\n7. Model ensembling\n8. Prediction and submission\n\n\n#### Computing environment setup\n1. Use Google Cloud or Amazon AWS as computing platform\n\n\n\n#### Exploratory data analysis \n1. Calculate summary statistics.  \n-. total number of samples and variables  \n-. number of missing values and zeros  \n-. mean, sd, min, max values for continuous variables  \n-. number of unqiue values/categories for categorial and ordinal variables  \n2. Plot \n\n\n  \n#### A quick benchmark run\n1. Use Random Forst (100 trees) without any feature enginnering to generate a quick submission. This submission can be used as a benchmark for further improvement. Plot the importance of the feaures to get a sense what are the most important features for prediction.\n2. Train a simple Random Forest model and plot the confusion matrix for classfication or true-prediction-value-scatter-plot for regression. Find out where most the prediction errors come from. For example, it may come from certain categories. Need to split original training data into training and testing data. \n\n#### Feature enginnering\n1. General transformation: multiply, divide, sum, subtract, log, min, max, mean, std \n2. If data have distance or length variables, several new features can be generated by multiplying (area or volume), dividing (ratio between two length), substrating (difference between two length) or summming (total length or distance) those variables or a subset of those variables. \n3. Date variable:\n(1) Extract day, month, quarter, year, weekend, weekday, holiday etc. as new features\n(2) Calculate the length between two dates\n\n#### Feature selection\n1. Use the feature importance generated by Random Forest or XGBoost to rank features. Iteratively remove the least importance features and fit the model until the accuracy of the prediction to decrease.\n2. Use XGBoost for feature selection:\n(1) Keep the number of trees small (<20 trees)\n(2) Keep the max depth of the tree small (<7)\n(3) Iteratively run the feature importance analysis by removing the most(or least) important features.\n3. Decision trees (XGBoost, Random Forest) are not affected multi-collinearity.\n\n\n#### Model evaluation and selection\n1. Popular models: Random Forst, Extra Trees, XGBoost\n\n#### Parameter turning\n1. Use grid search to fine tune parameters.\n2. For Random Forest and Extra Trees, two important parameters that can be tuned: number of trees and the number of randomly selected features to seek split.\n3. XGBoost:\n(1) small eta -> small shinkage -> less overfitting -> slow convergence -> need more trees\n\n---\n## Section two: Kaggle winner solutions and some useful tutorials\n\n1. [Beating Kaggle the easy way by Ying Dong](https://www.ke.tu-darmstadt.de/lehre/arbeiten/studien/2015/Dong_Ying.pdf)\n\n2. [How to get into the top 15 of a Kaggle competition using Python by Vik Paruchuri](https://www.dataquest.io/blog/kaggle-tutorial/)\n\n3. [Awesome XGBoost](https://github.com/dmlc/xgboost/blob/master/demo/README.md)\n\n4. [1st Place Solution for Search Results Relevance Competition on Kaggle](https://github.com/ChenglongChen/Kaggle_CrowdFlower)\n\n\n5. [Kaggle Competition Past Solutions](http://www.chioka.in/kaggle-competition-solutions/)\n\n6. [Learning Predictive Analytics: Kaggle Competition Solutions](http://analyticscosm.com/learning-predictive-analytics-kaggle-competition-solutions/)\n\n7. [No Free Hunch](http://blog.kaggle.com/)\n\n8. [Kaggle past solutions](https://www.kaggle.com/wiki/PastSolutions)\n\n9. [Winning solution of Kaggle Higgs competition: what a single model can do?](https://no2147483647.wordpress.com/2014/09/17/winning-solution-of-kaggle-higgs-competition-what-a-single-model-can-do/)\n\n10. [Feature Importance Analysis with XGBoost in Tax audit](http://www.slideshare.net/MichaelBENESTY/feature-importance-analysis-with-xgboost-in-tax-audit)\n\n11. [Write-up for Liberty Mutual's Hazard Prediction](http://jianghao.org/blog/20150905/liberty-mutual-group-hazard-prediction.html)\n\n12. [Understanding XGBoost Model on Otto Dataset](https://kaggle2.blob.core.windows.net/forum-message-attachments/76715/2435/Understanding%20XGBoost%20Model%20on%20Otto%20Dataset.html?sv=2015-12-11&sr=b&sig=zJn5WiSMSM7XHMYONF5LVnUt2E35AtqexBCUau9lTA0%3D&se=2016-12-09T02%3A10%3A50Z&sp=r)\n\n\n13. [Does XGBoost handle multicollinearity by itself?](http://datascience.stackexchange.com/questions/12554/does-xgboost-handle-multicollinearity-by-itself)\n\n14. [Bosch Production Line Performance Competition Winners' Interview: 3rd Place, Team Data Property Avengers | Darragh, Marios, Mathias, & Stanislav](http://blog.kaggle.com/2016/12/15/bosch-production-line-performance-competition-winners-interview-3rd-place-team-data-property-avengers-darragh-marios-mathias-stanislav/)\nmore\n",
    "created" : 1503463026740.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2459231422",
    "id" : "7133EA8E",
    "lastKnownWriteTime" : 1503474816,
    "last_content_update" : 1503474879914,
    "path" : "~/Sites/ty_analytic_workflow/README.md",
    "project_path" : "README.md",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "markdown"
}