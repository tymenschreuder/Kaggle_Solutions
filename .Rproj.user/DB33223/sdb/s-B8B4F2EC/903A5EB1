{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Tymen Preferred Flexible Analytic Process Guide\"\ngithub-repo: /tymenschreuder/ty_analytic_workflow\noutput:\n  html_output: html_document\n  pdf_output: pdf_document\ndescription: Intent is to serve 1) me, as knowledge base to remind me how I implemented\n  something in past 2) potentially develop into an educational guide for those learning\n  machine learning.\n---\n\n# My Own Personal Preference Analytic Workflow\n\n#### General workflow  \n1. Computing environment setup\n2. Exploratory data analysis \n2. A quick benchmark run\n2. Data\tpreprocessing\n3. Feature enginnering\n4. Feature selection\n5. Model evaluation and selection\n6. Parameter turning\n7. Model ensembling\n8. Prediction and submission\n\n#### Computing environment setup\n1. Use Google Cloud or Amazon AWS as computing platform\n\n#### Exploratory data analysis \n1. Calculate summary statistics.  \n-. total number of samples and variables  \n-. number of missing values and zeros  \n-. mean, sd, min, max values for continuous variables  \n-. number of unqiue values/categories for categorial and ordinal variables  \n2. Plot \n\n#### A quick benchmark run\n1. Use Random Forst (100 trees) without any feature enginnering to generate a quick submission. This submission can be used as a benchmark for further improvement. Plot the importance of the feaures to get a sense what are the most important features for prediction.\n2. Train a simple Random Forest model and plot the confusion matrix for classfication or true-prediction-value-scatter-plot for regression. Find out where most the prediction errors come from. For example, it may come from certain categories. Need to split original training data into training and testing data. \n\n#### Feature enginnering\n1. General transformation: multiply, divide, sum, subtract, log, min, max, mean, std \n2. If data have distance or length variables, several new features can be generated by multiplying (area or volume), dividing (ratio between two length), substrating (difference between two length) or summming (total length or distance) those variables or a subset of those variables. \n3. Date variable:\n(1) Extract day, month, quarter, year, weekend, weekday, holiday etc. as new features\n(2) Calculate the length between two dates\n\n#### Feature selection\n1. Use the feature importance generated by Random Forest or XGBoost to rank features. Iteratively remove the least importance features and fit the model until the accuracy of the prediction to decrease.\n2. Use XGBoost for feature selection:\n(1) Keep the number of trees small (<20 trees)\n(2) Keep the max depth of the tree small (<7)\n(3) Iteratively run the feature importance analysis by removing the most(or least) important features.\n3. Decision trees (XGBoost, Random Forest) are not affected multi-collinearity.\n\n\n#### Model evaluation and selection\n1. Popular models: Random Forst, Extra Trees, XGBoost\n\n#### Parameter turning\n1. Use grid search to fine tune parameters.\n2. For Random Forest and Extra Trees, two important parameters that can be tuned: number of trees and the number of randomly selected features to seek split.\n3. XGBoost:\n(1) small eta -> small shinkage -> less overfitting -> slow convergence -> need more trees\n",
    "created" : 1503479138489.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2936486494",
    "id" : "903A5EB1",
    "lastKnownWriteTime" : 1503479727,
    "last_content_update" : 1503479727302,
    "path" : "~/Sites/ty_analytic_workflow/workflowTemplate.Rmd",
    "project_path" : "workflowTemplate.Rmd",
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}